{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 463 samples, validate on 116 samples\n",
      "Epoch 1/40\n",
      "463/463 [==============================] - 2s 3ms/step - loss: 0.2307 - acc: 0.6523 - val_loss: 0.1921 - val_acc: 0.7414\n",
      "Epoch 2/40\n",
      "463/463 [==============================] - 0s 183us/step - loss: 0.2083 - acc: 0.7084 - val_loss: 0.1917 - val_acc: 0.7414\n",
      "Epoch 3/40\n",
      "463/463 [==============================] - 0s 155us/step - loss: 0.2080 - acc: 0.7084 - val_loss: 0.1919 - val_acc: 0.7414\n",
      "Epoch 4/40\n",
      "463/463 [==============================] - 0s 190us/step - loss: 0.2092 - acc: 0.7084 - val_loss: 0.1921 - val_acc: 0.7414\n",
      "Epoch 5/40\n",
      "463/463 [==============================] - 0s 174us/step - loss: 0.2116 - acc: 0.7084 - val_loss: 0.1918 - val_acc: 0.7414\n",
      "Epoch 6/40\n",
      "463/463 [==============================] - 0s 153us/step - loss: 0.2085 - acc: 0.7084 - val_loss: 0.1957 - val_acc: 0.7414\n",
      "Epoch 7/40\n",
      "463/463 [==============================] - 0s 127us/step - loss: 0.2110 - acc: 0.7084 - val_loss: 0.2021 - val_acc: 0.7414\n",
      "Epoch 8/40\n",
      "463/463 [==============================] - 0s 140us/step - loss: 0.2099 - acc: 0.7084 - val_loss: 0.1940 - val_acc: 0.7414\n",
      "Epoch 9/40\n",
      "463/463 [==============================] - 0s 194us/step - loss: 0.2080 - acc: 0.7084 - val_loss: 0.1917 - val_acc: 0.7414\n",
      "Epoch 10/40\n",
      "463/463 [==============================] - 0s 155us/step - loss: 0.2079 - acc: 0.7084 - val_loss: 0.1916 - val_acc: 0.7414\n",
      "Epoch 11/40\n",
      "463/463 [==============================] - 0s 169us/step - loss: 0.2073 - acc: 0.7084 - val_loss: 0.1947 - val_acc: 0.7414\n",
      "Epoch 12/40\n",
      "463/463 [==============================] - 0s 163us/step - loss: 0.2078 - acc: 0.7084 - val_loss: 0.1937 - val_acc: 0.7414\n",
      "Epoch 13/40\n",
      "463/463 [==============================] - 0s 125us/step - loss: 0.2073 - acc: 0.7084 - val_loss: 0.1916 - val_acc: 0.7414\n",
      "Epoch 14/40\n",
      "463/463 [==============================] - 0s 121us/step - loss: 0.2076 - acc: 0.7084 - val_loss: 0.1935 - val_acc: 0.7414\n",
      "Epoch 15/40\n",
      "463/463 [==============================] - 0s 136us/step - loss: 0.2079 - acc: 0.7084 - val_loss: 0.1915 - val_acc: 0.7414\n",
      "Epoch 16/40\n",
      "463/463 [==============================] - 0s 131us/step - loss: 0.2059 - acc: 0.7084 - val_loss: 0.1962 - val_acc: 0.7414\n",
      "Epoch 17/40\n",
      "463/463 [==============================] - 0s 112us/step - loss: 0.2083 - acc: 0.7084 - val_loss: 0.1917 - val_acc: 0.7414\n",
      "Epoch 18/40\n",
      "463/463 [==============================] - 0s 125us/step - loss: 0.2078 - acc: 0.7084 - val_loss: 0.1907 - val_acc: 0.7414\n",
      "Epoch 19/40\n",
      "463/463 [==============================] - 0s 138us/step - loss: 0.2066 - acc: 0.7084 - val_loss: 0.1913 - val_acc: 0.7414\n",
      "Epoch 20/40\n",
      "463/463 [==============================] - 0s 164us/step - loss: 0.2076 - acc: 0.7084 - val_loss: 0.1954 - val_acc: 0.7414\n",
      "Epoch 21/40\n",
      "463/463 [==============================] - 0s 146us/step - loss: 0.2070 - acc: 0.7084 - val_loss: 0.1897 - val_acc: 0.7414\n",
      "Epoch 22/40\n",
      "463/463 [==============================] - 0s 168us/step - loss: 0.2064 - acc: 0.7084 - val_loss: 0.1890 - val_acc: 0.7414\n",
      "Epoch 23/40\n",
      "463/463 [==============================] - 0s 179us/step - loss: 0.2068 - acc: 0.7084 - val_loss: 0.1893 - val_acc: 0.7414\n",
      "Epoch 24/40\n",
      "463/463 [==============================] - 0s 153us/step - loss: 0.2049 - acc: 0.7084 - val_loss: 0.1999 - val_acc: 0.7414\n",
      "Epoch 25/40\n",
      "463/463 [==============================] - 0s 157us/step - loss: 0.2145 - acc: 0.7084 - val_loss: 0.1840 - val_acc: 0.7414\n",
      "Epoch 26/40\n",
      "463/463 [==============================] - 0s 151us/step - loss: 0.2044 - acc: 0.7084 - val_loss: 0.1820 - val_acc: 0.7414\n",
      "Epoch 27/40\n",
      "463/463 [==============================] - 0s 174us/step - loss: 0.1998 - acc: 0.7084 - val_loss: 0.1779 - val_acc: 0.7414\n",
      "Epoch 28/40\n",
      "463/463 [==============================] - 0s 166us/step - loss: 0.1985 - acc: 0.7084 - val_loss: 0.1739 - val_acc: 0.7414\n",
      "Epoch 29/40\n",
      "463/463 [==============================] - 0s 154us/step - loss: 0.1971 - acc: 0.7084 - val_loss: 0.1672 - val_acc: 0.7414\n",
      "Epoch 30/40\n",
      "463/463 [==============================] - 0s 190us/step - loss: 0.1961 - acc: 0.7084 - val_loss: 0.1643 - val_acc: 0.7414\n",
      "Epoch 31/40\n",
      "463/463 [==============================] - 0s 146us/step - loss: 0.1927 - acc: 0.7084 - val_loss: 0.1585 - val_acc: 0.7414\n",
      "Epoch 32/40\n",
      "463/463 [==============================] - 0s 194us/step - loss: 0.1892 - acc: 0.7106 - val_loss: 0.1546 - val_acc: 0.7414\n",
      "Epoch 33/40\n",
      "463/463 [==============================] - 0s 183us/step - loss: 0.1878 - acc: 0.7041 - val_loss: 0.1526 - val_acc: 0.7500\n",
      "Epoch 34/40\n",
      "463/463 [==============================] - 0s 162us/step - loss: 0.1857 - acc: 0.7106 - val_loss: 0.1532 - val_acc: 0.7672\n",
      "Epoch 35/40\n",
      "463/463 [==============================] - 0s 196us/step - loss: 0.1829 - acc: 0.6998 - val_loss: 0.1604 - val_acc: 0.7414\n",
      "Epoch 36/40\n",
      "463/463 [==============================] - 0s 190us/step - loss: 0.1926 - acc: 0.7019 - val_loss: 0.1627 - val_acc: 0.7414\n",
      "Epoch 37/40\n",
      "463/463 [==============================] - 0s 185us/step - loss: 0.1870 - acc: 0.7106 - val_loss: 0.1478 - val_acc: 0.7414\n",
      "Epoch 38/40\n",
      "463/463 [==============================] - 0s 166us/step - loss: 0.1843 - acc: 0.7084 - val_loss: 0.1480 - val_acc: 0.7414\n",
      "Epoch 39/40\n",
      "463/463 [==============================] - 0s 183us/step - loss: 0.1837 - acc: 0.7019 - val_loss: 0.1457 - val_acc: 0.7414\n",
      "Epoch 40/40\n",
      "463/463 [==============================] - 0s 183us/step - loss: 0.1880 - acc: 0.6955 - val_loss: 0.1621 - val_acc: 0.7672\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = np.genfromtxt(\"datasets/pima/pima_x.csv\", delimiter=\",\")\n",
    "y = np.genfromtxt(\"datasets/pima/pima_y.csv\", delimiter=\",\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(120, input_dim=len(X[0]), activation=\"sigmoid\"))\n",
    "model.add(Dense(105, activation=\"sigmoid\"))\n",
    "model.add(Dense(95, activation=\"sigmoid\"))\n",
    "model.add(Dense(91, activation=\"sigmoid\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "hist = model.fit(X, y, epochs=40, batch_size=16, verbose=1, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.19214763158354267,\n",
       " 0.19171639917225675,\n",
       " 0.19188207131007623,\n",
       " 0.19210207051244274,\n",
       " 0.19183564340246134,\n",
       " 0.19566572534626928,\n",
       " 0.20211778940825625,\n",
       " 0.19399831418333383,\n",
       " 0.19168641649443527,\n",
       " 0.19155337481663146,\n",
       " 0.19470327270442042,\n",
       " 0.19369225871974025,\n",
       " 0.19159101206680823,\n",
       " 0.1934723319678471,\n",
       " 0.19147432826716324,\n",
       " 0.19620372098067712,\n",
       " 0.19174484240597692,\n",
       " 0.19073940505241527,\n",
       " 0.1912858208705639,\n",
       " 0.19535697225866647,\n",
       " 0.18969271100800614,\n",
       " 0.18896265173780508,\n",
       " 0.18927993651094108,\n",
       " 0.19994083869046178,\n",
       " 0.1840210711133891,\n",
       " 0.18196078210041441,\n",
       " 0.17785270614870663,\n",
       " 0.1738864121765926,\n",
       " 0.16720995409735318,\n",
       " 0.16426527602919216,\n",
       " 0.15848000398997603,\n",
       " 0.1545657125012628,\n",
       " 0.15256505382472071,\n",
       " 0.15317967945131764,\n",
       " 0.16042098711276875,\n",
       " 0.16266970593353797,\n",
       " 0.14780136840096836,\n",
       " 0.1479774559366292,\n",
       " 0.14569295685866784,\n",
       " 0.1621042315302224]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 463 samples, validate on 116 samples\n",
      "Epoch 1/40\n",
      "463/463 [==============================] - 1s 3ms/step - loss: 0.6099 - acc: 0.7084 - val_loss: 0.5826 - val_acc: 0.7414\n",
      "Epoch 2/40\n",
      "463/463 [==============================] - 0s 144us/step - loss: 0.6049 - acc: 0.7084 - val_loss: 0.5761 - val_acc: 0.7414\n",
      "Epoch 3/40\n",
      "463/463 [==============================] - 0s 121us/step - loss: 0.6039 - acc: 0.7084 - val_loss: 0.5746 - val_acc: 0.7414\n",
      "Epoch 4/40\n",
      "463/463 [==============================] - 0s 117us/step - loss: 0.6038 - acc: 0.7084 - val_loss: 0.5746 - val_acc: 0.7414\n",
      "Epoch 5/40\n",
      "463/463 [==============================] - 0s 134us/step - loss: 0.6038 - acc: 0.7084 - val_loss: 0.5746 - val_acc: 0.7414\n",
      "Epoch 6/40\n",
      "463/463 [==============================] - 0s 151us/step - loss: 0.6039 - acc: 0.7084 - val_loss: 0.5735 - val_acc: 0.7414\n",
      "Epoch 7/40\n",
      "463/463 [==============================] - 0s 118us/step - loss: 0.6035 - acc: 0.7084 - val_loss: 0.5742 - val_acc: 0.7414\n",
      "Epoch 8/40\n",
      "463/463 [==============================] - 0s 155us/step - loss: 0.6035 - acc: 0.7084 - val_loss: 0.5743 - val_acc: 0.7414\n",
      "Epoch 9/40\n",
      "463/463 [==============================] - 0s 209us/step - loss: 0.6040 - acc: 0.7084 - val_loss: 0.5734 - val_acc: 0.7414\n",
      "Epoch 10/40\n",
      "463/463 [==============================] - 0s 179us/step - loss: 0.6037 - acc: 0.7084 - val_loss: 0.5741 - val_acc: 0.7414\n",
      "Epoch 11/40\n",
      "463/463 [==============================] - 0s 200us/step - loss: 0.6047 - acc: 0.7084 - val_loss: 0.5739 - val_acc: 0.7414\n",
      "Epoch 12/40\n",
      "463/463 [==============================] - 0s 172us/step - loss: 0.6033 - acc: 0.7084 - val_loss: 0.5723 - val_acc: 0.7414\n",
      "Epoch 13/40\n",
      "463/463 [==============================] - 0s 159us/step - loss: 0.6033 - acc: 0.7084 - val_loss: 0.5723 - val_acc: 0.7414\n",
      "Epoch 14/40\n",
      "463/463 [==============================] - 0s 162us/step - loss: 0.6034 - acc: 0.7084 - val_loss: 0.5734 - val_acc: 0.7414\n",
      "Epoch 15/40\n",
      "463/463 [==============================] - 0s 238us/step - loss: 0.6031 - acc: 0.7084 - val_loss: 0.5724 - val_acc: 0.7414\n",
      "Epoch 16/40\n",
      "463/463 [==============================] - 0s 182us/step - loss: 0.6029 - acc: 0.7084 - val_loss: 0.5725 - val_acc: 0.7414\n",
      "Epoch 17/40\n",
      "463/463 [==============================] - 0s 201us/step - loss: 0.6028 - acc: 0.7084 - val_loss: 0.5722 - val_acc: 0.7414\n",
      "Epoch 18/40\n",
      "463/463 [==============================] - 0s 192us/step - loss: 0.6030 - acc: 0.7084 - val_loss: 0.5718 - val_acc: 0.7414\n",
      "Epoch 19/40\n",
      "463/463 [==============================] - 0s 127us/step - loss: 0.6027 - acc: 0.7084 - val_loss: 0.5726 - val_acc: 0.7414\n",
      "Epoch 20/40\n",
      "463/463 [==============================] - 0s 170us/step - loss: 0.6025 - acc: 0.7084 - val_loss: 0.5725 - val_acc: 0.7414\n",
      "Epoch 21/40\n",
      "463/463 [==============================] - 0s 127us/step - loss: 0.6023 - acc: 0.7084 - val_loss: 0.5719 - val_acc: 0.7414\n",
      "Epoch 22/40\n",
      "463/463 [==============================] - 0s 127us/step - loss: 0.6027 - acc: 0.7084 - val_loss: 0.5715 - val_acc: 0.7414\n",
      "Epoch 23/40\n",
      "463/463 [==============================] - 0s 121us/step - loss: 0.6024 - acc: 0.7084 - val_loss: 0.5709 - val_acc: 0.7414\n",
      "Epoch 24/40\n",
      "463/463 [==============================] - 0s 112us/step - loss: 0.6027 - acc: 0.7084 - val_loss: 0.5703 - val_acc: 0.7414\n",
      "Epoch 25/40\n",
      "463/463 [==============================] - 0s 202us/step - loss: 0.6017 - acc: 0.7084 - val_loss: 0.5708 - val_acc: 0.7414\n",
      "Epoch 26/40\n",
      "463/463 [==============================] - 0s 159us/step - loss: 0.6018 - acc: 0.7084 - val_loss: 0.5706 - val_acc: 0.7414\n",
      "Epoch 27/40\n",
      "463/463 [==============================] - 0s 168us/step - loss: 0.6017 - acc: 0.7084 - val_loss: 0.5699 - val_acc: 0.7414\n",
      "Epoch 28/40\n",
      "463/463 [==============================] - 0s 217us/step - loss: 0.6014 - acc: 0.7084 - val_loss: 0.5706 - val_acc: 0.7414\n",
      "Epoch 29/40\n",
      "463/463 [==============================] - 0s 123us/step - loss: 0.6015 - acc: 0.7084 - val_loss: 0.5699 - val_acc: 0.7414\n",
      "Epoch 30/40\n",
      "463/463 [==============================] - 0s 144us/step - loss: 0.6014 - acc: 0.7084 - val_loss: 0.5699 - val_acc: 0.7414\n",
      "Epoch 31/40\n",
      "463/463 [==============================] - 0s 151us/step - loss: 0.6010 - acc: 0.7084 - val_loss: 0.5693 - val_acc: 0.7414\n",
      "Epoch 32/40\n",
      "463/463 [==============================] - 0s 168us/step - loss: 0.6008 - acc: 0.7084 - val_loss: 0.5692 - val_acc: 0.7414\n",
      "Epoch 33/40\n",
      "463/463 [==============================] - 0s 125us/step - loss: 0.6009 - acc: 0.7084 - val_loss: 0.5682 - val_acc: 0.7414\n",
      "Epoch 34/40\n",
      "463/463 [==============================] - 0s 129us/step - loss: 0.6004 - acc: 0.7084 - val_loss: 0.5677 - val_acc: 0.7414\n",
      "Epoch 35/40\n",
      "463/463 [==============================] - 0s 168us/step - loss: 0.6005 - acc: 0.7084 - val_loss: 0.5678 - val_acc: 0.7414\n",
      "Epoch 36/40\n",
      "463/463 [==============================] - 0s 140us/step - loss: 0.6001 - acc: 0.7084 - val_loss: 0.5669 - val_acc: 0.7414\n",
      "Epoch 37/40\n",
      "463/463 [==============================] - 0s 153us/step - loss: 0.5998 - acc: 0.7084 - val_loss: 0.5663 - val_acc: 0.7414\n",
      "Epoch 38/40\n",
      "463/463 [==============================] - 0s 164us/step - loss: 0.5995 - acc: 0.7084 - val_loss: 0.5658 - val_acc: 0.7414\n",
      "Epoch 39/40\n",
      "463/463 [==============================] - 0s 153us/step - loss: 0.5996 - acc: 0.7084 - val_loss: 0.5664 - val_acc: 0.7414\n",
      "Epoch 40/40\n",
      "463/463 [==============================] - 0s 144us/step - loss: 0.5989 - acc: 0.7084 - val_loss: 0.5649 - val_acc: 0.7414\n"
     ]
    }
   ],
   "source": [
    "model_baseline = Sequential()\n",
    "model_baseline.add(Dense(10, input_dim=len(X[0]), activation=\"sigmoid\"))\n",
    "model_baseline.add(Dense(10, activation=\"sigmoid\"))\n",
    "model_baseline.add(Dense(10, activation=\"sigmoid\"))\n",
    "model_baseline.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_baseline.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hist_baseline = model_baseline.fit(X, y, epochs=40, batch_size=16, verbose=1, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5826238126590334,\n",
       " 0.5760767624295992,\n",
       " 0.5745994263681872,\n",
       " 0.574637795316762,\n",
       " 0.5746491160886041,\n",
       " 0.5735458505564722,\n",
       " 0.5741771624006075,\n",
       " 0.5742690563201904,\n",
       " 0.5733623237445437,\n",
       " 0.5741052874203386,\n",
       " 0.5739333321308268,\n",
       " 0.5723233099641472,\n",
       " 0.5723076668278925,\n",
       " 0.57340132368022,\n",
       " 0.5724433208334034,\n",
       " 0.5725239844157778,\n",
       " 0.5721804479072834,\n",
       " 0.5718426581086784,\n",
       " 0.5726350217029966,\n",
       " 0.5724654567652735,\n",
       " 0.5718979013377222,\n",
       " 0.571527518075088,\n",
       " 0.5709097220979887,\n",
       " 0.5703101425335325,\n",
       " 0.5708184550548422,\n",
       " 0.5706258190089258,\n",
       " 0.5699307466375416,\n",
       " 0.5705794342632952,\n",
       " 0.5698592148978134,\n",
       " 0.5698641990793163,\n",
       " 0.5692911353604547,\n",
       " 0.569232375457369,\n",
       " 0.5682149138943903,\n",
       " 0.5676527311061991,\n",
       " 0.5677724336755687,\n",
       " 0.5669356707868904,\n",
       " 0.5663225013634254,\n",
       " 0.565805381742017,\n",
       " 0.5663891044156305,\n",
       " 0.5649217223298961]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_baseline.history[\"val_loss\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
